{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Video diffusion models","metadata":{}},{"cell_type":"code","source":"!wget https://www.kaggleusercontent.com/kf/116808580/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..jsredvRKL5mb488KjnL_ag.Q6U8Eq6OWh4aijOVHH_YHitw8nG5nzKNuviS8PRAFG0HHi0fKQsU_CbQJc3rsT0w-u2eh_yG4hLX6qcIe7vEY3KmCSusbmxHFjjXah5Sl3CrU0pkDdH66MunAuEBExsPiITkJej3kjisldzwzKHbewtykhcj_sqclyXqLc4rATXfkNy6O6BhLwuiyzQrN98VlcMpUgBCDXLJvew38DZ8MtqA0GsKFjVfA_FZjxfVBetPeOGogWuraTBfNV4b5alWjSf0NekBqE2tB1MOQ2889KCUwgeo57zMEUeyoV45Id30BfHWuClTMpnLxg3xfzEh03OgKWvrLXCnm1Xt4lJG1v1jyZirKLNHE_WAjb5emN_Xu96iQxX920HeFfB4DddxzbBuptjdnZnp0Ln9wZW0UMPVUN_JvgMM4IcwqQ6yb1jq7FS1R7SkfUR7HjMBIF53wZFW7-SzAI3Auer3qxs_bkU6xZk_XQfHTdjeGiCOv6M_2BlTFW6kXmqR5erSUPi9-kqwcg7ZDwVK3Znu9SRoqSyzW1Ja0eSbpz7hFsW6W3YtpKpQivpqWPslGMCfRGMMWIa1XUCI6wrYFB-iXtmlvc8cVug4zI4XlgJDRdJwS225qy9n6TCRQtQ18S5dKNYTq3x-e1hzE35aAGik6Y_SNA.-wfOeyfpHsSR9AZlhtEuCw/checkpoint-e1_35-s1_600000-e2_36-s2_29695-1674197094.pt","metadata":{"execution":{"iopub.status.busy":"2023-01-19T18:15:36.639123Z","iopub.execute_input":"2023-01-19T18:15:36.639767Z","iopub.status.idle":"2023-01-19T18:16:33.258947Z","shell.execute_reply.started":"2023-01-19T18:15:36.639723Z","shell.execute_reply":"2023-01-19T18:16:33.257814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Params\nimage_size = 128\nframes = 12\nprocess_batch_size = 128\n\nmax_run_minutes = 12 * 60 - 10","metadata":{"execution":{"iopub.status.busy":"2023-01-19T18:16:33.260996Z","iopub.execute_input":"2023-01-19T18:16:33.261408Z","iopub.status.idle":"2023-01-19T18:16:33.266923Z","shell.execute_reply.started":"2023-01-19T18:16:33.261366Z","shell.execute_reply":"2023-01-19T18:16:33.265993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport os\nstart_time = time.time()","metadata":{"execution":{"iopub.status.busy":"2023-01-19T18:16:33.269927Z","iopub.execute_input":"2023-01-19T18:16:33.270678Z","iopub.status.idle":"2023-01-19T18:16:33.419142Z","shell.execute_reply.started":"2023-01-19T18:16:33.270642Z","shell.execute_reply":"2023-01-19T18:16:33.418186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Install dependencies","metadata":{}},{"cell_type":"code","source":"!pip install imagen_pytorch==1.16.5 --no-cache-dir","metadata":{"execution":{"iopub.status.busy":"2023-01-19T18:16:33.422349Z","iopub.execute_input":"2023-01-19T18:16:33.423305Z","iopub.status.idle":"2023-01-19T18:16:55.862780Z","shell.execute_reply.started":"2023-01-19T18:16:33.423264Z","shell.execute_reply":"2023-01-19T18:16:55.861584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utility functions to resize and crop GIFs","metadata":{}},{"cell_type":"code","source":"# GIF pre-processing\n\nimport numpy as np\nfrom torchvision import transforms as T\nfrom math import floor, fabs\nfrom PIL import Image, ImageSequence\n\n\nCHANNELS_TO_MODE = {\n    1 : 'L',\n    3 : 'RGB',\n    4 : 'RGBA'\n}\n\ndef center_crop(img, new_width, new_height): \n    width = img.size[0]\n    height = img.size[1]\n    left = int(np.ceil((width - new_width) / 2))\n    right = width - int(np.floor((width - new_width) / 2))\n    top = int(np.ceil((height - new_height) / 2))\n    bottom = height - int(np.floor((height - new_height) / 2))\n    return img.crop((left, top, right, bottom))\n\ndef resize_crop_img(img, width, height):\n    # width < height\n    if( img.size[0] < img.size[1]):\n        wpercent = (width/float(img.size[0]))\n        hsize = int((float(img.size[1])*float(wpercent)))\n        img = img.resize((width, hsize), Image.Resampling.LANCZOS)\n    else: # width >= height\n        hpercent = (height/float(img.size[1]))\n        wsize = int((float(img.size[0])*float(hpercent)))\n        img = img.resize((wsize, height), Image.Resampling.LANCZOS)\n    img = center_crop(img, width, height)\n    return img\n\ndef transform_gif(img, new_width, new_height, frames, channels = 3):\n    assert channels in CHANNELS_TO_MODE, f'channels {channels} invalid'\n    mode = CHANNELS_TO_MODE[channels]\n    gif_frames = img.n_frames\n    for i in range(0, frames):\n        img.seek(i % gif_frames)\n        if img.size[0] != new_width or img.size[0] != new_width:\n            print(\"Resizing\")\n            img_out = resize_crop_img(img, new_width, new_height)\n        else:\n            img_out = img\n        yield img_out.convert(mode)\n        \n# tensor of shape (channels, frames, height, width) -> gif\ndef video_tensor_to_gif(tensor, path, fps = 10, loop = 0, optimize = True):\n    print(\"Converting video tensors to GIF\")\n    images = map(T.ToPILImage(), tensor.unbind(dim = 1))\n    first_img, *rest_imgs = images\n    print(1000/fps)\n    first_img.save(path, save_all = True, append_images = rest_imgs, duration = int(1000/fps), loop = loop, optimize = optimize)\n    print(\"Gif saved\")\n    return images\n\n# gif -> (channels, frame, height, width) tensor\ndef gif_to_tensor(path, width = 256, height = 256, frames = 32, channels = 3, transform = T.ToTensor()):\n    print(\"Converting GIF to video tensors\")\n    img = Image.open(path)\n    imgs = transform_gif(img, new_width = width, new_height = height, frames = frames, channels = channels)\n    tensors = tuple(map(transform, imgs))\n    return torch.stack(tensors, dim = 1)\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-01-19T18:16:55.864610Z","iopub.execute_input":"2023-01-19T18:16:55.865028Z","iopub.status.idle":"2023-01-19T18:16:55.884641Z","shell.execute_reply.started":"2023-01-19T18:16:55.864956Z","shell.execute_reply":"2023-01-19T18:16:55.883663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utility functions to process dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport shutil\nimport urllib\nimport traceback\n\nfrom concurrent.futures import ThreadPoolExecutor, wait\nimport time\nimport threading\n\ndataset_size = 600000\ntrain_data = \"../input/webvid-gif-600k/dataset_600K.tsv\"\ndata_dir = \"../input/webvid-gif-600k/data/\"\n\ncurrent_step = 0\ntexts = []\nlist_videos = []\n\n\nlock = threading.Lock()\nexecutor = ThreadPoolExecutor(max_workers=15)\n\ndef process_parallel(index, file_img, file_text):\n    try:\n        print(f\"Processing image {file_img}\")\n        tensor = gif_to_tensor(data_dir + file_img, width = image_size, height = image_size, frames = frames)\n        file_text = file_text[:-1] # Remove \\n\n        with lock:\n            list_videos.append(tensor)\n            texts.append(file_text)\n    except:\n        traceback.print_exc()\n    \ndef get_videos_parallel(index_start, index_end):\n    global texts\n    global list_videos\n    \n    texts = []\n    list_videos = []\n\n    with open(train_data) as fp:\n        futures = []\n        for i, line in enumerate(fp):\n            if i >= index_start and i< index_end :\n                file_img, file_text = line.split(\"\\t\")\n                future = executor.submit(process_parallel, i, file_img, file_text)\n                futures.append(future)\n            elif i > index_end:\n                break\n        wait(futures)\n                \ndef get_next_videos():\n    global current_step\n    get_videos_parallel(current_step, current_step + process_batch_size)\n    current_step += len(texts)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-19T18:16:55.886460Z","iopub.execute_input":"2023-01-19T18:16:55.886868Z","iopub.status.idle":"2023-01-19T18:16:55.898702Z","shell.execute_reply.started":"2023-01-19T18:16:55.886833Z","shell.execute_reply":"2023-01-19T18:16:55.897659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utility functions to save and load checkpoints","metadata":{}},{"cell_type":"code","source":"import shutil\nimport torch\nimport time\nimport gc\nimport os\nfrom imagen_pytorch import Unet3D, ElucidatedImagen, ImagenTrainer\nfrom imagen_pytorch.data import Dataset\n\ncheckpoints_path = \"./\"\ncheckpoint_path = \"\"\n\n# If there is a checkpoint these changes automatically at runtime\nepoch_1 = 0\nepoch_2 = 0\n\nstep_1 = 0\nstep_2 = 0\n\ntrain_unet = 1\n\ndef save_checkpoint(trainer: ImagenTrainer):\n    global checkpoint_path\n    global current_step\n    global epoch_1\n    global epoch_2\n    global step_1\n    global step_2\n    print(\"Saving checkpoint\")\n    current_time = int(time.time())\n    if os.path.exists(checkpoint_path):\n        os.remove(checkpoint_path)\n    if train_unet == 1:\n        step_1 = current_step\n    else:\n        step_2 = current_step\n    checkpoint_path = os.path.join(checkpoints_path, f\"checkpoint-e1_{epoch_1}-s1_{step_1}-e2_{epoch_2}-s2_{step_2}-{current_time}.pt\")\n    trainer.save(checkpoint_path)\n\ndef update_config(checkpoint):\n    global epoch_1\n    global epoch_2\n    global step_1\n    global step_2\n    global train_unet\n    global current_step\n    splitted = (checkpoint.replace(\".pt\", \"\").split(\"checkpoint-\")[1]).split(\"-\")\n    epoch_1 = int(splitted[0].replace(\"e1_\", \"\"))\n    step_1 = int(splitted[1].replace(\"s1_\", \"\"))\n    epoch_2 = int(splitted[2].replace(\"e2_\", \"\"))\n    step_2 = int(splitted[3].replace(\"s2_\", \"\"))\n    print(\"Loaded configuration\")\n    print(f\"Epoch unet 1: {epoch_1}\")\n    print(f\"Steps unet 1: {step_1}\")\n    print(f\"Epoch unet 2: {epoch_2}\")\n    print(f\"Steps unet 2: {step_2}\")\n    if epoch_2 >= epoch_1:\n        train_unet = 1\n        if step_1 >= dataset_size:\n            current_step = 0\n            epoch_1 += 1\n        else:\n            current_step = step_1\n    else:\n        train_unet = 2\n        if step_2 >= dataset_size:\n            current_step = 0\n            epoch_2 += 1\n        else:\n            current_step = step_2\n    print(f\"Unet {train_unet} selected\")\n    print(f\"Current step: {current_step}\")\n    \ndef config_new_epoch():\n    global epoch_1\n    global epoch_2\n    global train_unet\n    global current_step\n    if train_unet == 1:\n        epoch_1 += 1\n    else:\n        epoch_2 += 1\n    current_step = 0\n        \ndef load_checkpoint(trainer: ImagenTrainer):\n    global checkpoint_path\n    global epoch_1\n    global epoch_2\n    global step_1\n    global step_2\n    global train_unet\n    global current_step\n    print(\"Loading checkpoint\")\n    timestamp = -1\n    for file in os.listdir(checkpoints_path):\n        if file.endswith('.pt'):\n            new_timestamp = int((file.split(\"-\")[-1]).replace(\".pt\", \"\"))\n            if new_timestamp > timestamp:\n                checkpoint_path = os.path.join(checkpoints_path, file)\n                timestamp = new_timestamp        \n    if not os.path.exists(checkpoint_path):\n        print(\"No checkpoint found -> starting from scratch\")\n        epoch_1 = 0\n        epoch_2 = 0\n        step_1 = 0\n        step_2 = 0\n        current_step = 0\n        train_unet = 1\n        return None\n    trainer.load(checkpoint_path)\n    update_config(checkpoint_path)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-01-19T18:16:58.733884Z","iopub.execute_input":"2023-01-19T18:16:58.734298Z","iopub.status.idle":"2023-01-19T18:16:58.752800Z","shell.execute_reply.started":"2023-01-19T18:16:58.734264Z","shell.execute_reply":"2023-01-19T18:16:58.751850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unet1 = Unet3D(\n    dim = 64,\n    cond_dim = 128,\n    dim_mults = (1, 2, 4, 8),\n    num_resnet_blocks = 3,\n    layer_attns = (False, True, True, True),\n    layer_cross_attns = (False, True, True, True)\n)\n\nunet2 = Unet3D(\n    dim = 64,\n    cond_dim = 128,\n    dim_mults = (1, 2, 4, 8),\n    num_resnet_blocks = (2, 4, 8, 8),\n    layer_attns = (False, False, False, True),\n    layer_cross_attns = (False, False, False, True)\n)\n\nimagen = ElucidatedImagen(\n    unets = (unet1, unet2),\n    image_sizes = (16, 64),\n    random_crop_sizes = (None, 16),\n    num_sample_steps = 64,\n    cond_drop_prob = 0.1,                       # gives the probability of dropout for classifier-free guidance.\n    sigma_min = 0.002,                          # min noise level\n    sigma_max = (80, 160),                      # max noise level, double the max noise level for upsampler\n    sigma_data = 0.5,                           # standard deviation of data distribution\n    rho = 7,                                    # controls the sampling schedule\n    P_mean = -1.2,                              # mean of log-normal distribution from which noise is drawn for training\n    P_std = 1.2,                                # standard deviation of log-normal distribution from which noise is drawn for training\n    S_churn = 80,                               # parameters for stochastic sampling - depends on dataset, Table 5 in apper\n    S_tmin = 0.05,\n    S_tmax = 50,\n    S_noise = 1.003,\n).cuda()\n\ntrainer = ImagenTrainer(imagen)\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-01-19T18:16:58.754543Z","iopub.execute_input":"2023-01-19T18:16:58.754789Z","iopub.status.idle":"2023-01-19T18:17:05.682052Z","shell.execute_reply.started":"2023-01-19T18:16:58.754759Z","shell.execute_reply":"2023-01-19T18:17:05.681051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Unet","metadata":{}},{"cell_type":"code","source":"# Train \nload_checkpoint(trainer)\nwhile True:\n    # if execution time is more than max_run_minutes stops\n    if time.time() - start_time >= max_run_minutes * 60:\n        break\n    get_next_videos()\n    if len(texts) == 0:\n        save_checkpoint(trainer)\n        config_new_epoch()\n        get_next_videos()\n#         break\n    print(\"Generating tensor from videos\")\n    videos = torch.stack(list_videos, dim = 0).cuda()\n    print(f\"Training Unet {train_unet}\")\n    trainer(videos, texts = texts, unet_number = train_unet, max_batch_size = 32)\n    trainer.update(unet_number = train_unet)\n    del videos\nsave_checkpoint(trainer)","metadata":{"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2023-01-18T22:23:16.921900Z","iopub.execute_input":"2023-01-18T22:23:16.922890Z","iopub.status.idle":"2023-01-18T22:23:29.251487Z","shell.execute_reply.started":"2023-01-18T22:23:16.922846Z","shell.execute_reply":"2023-01-18T22:23:29.249884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts_sample = ['A red cat']\nload_checkpoint(trainer)\nvideos_out = trainer.sample(texts = texts_sample, video_frames = 24)\nprint(videos_out.shape)\nvideo_tensor_to_gif(videos_out[0], f'out.gif', fps = 5)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T18:17:05.685887Z","iopub.execute_input":"2023-01-19T18:17:05.686230Z","iopub.status.idle":"2023-01-19T18:19:07.069351Z","shell.execute_reply.started":"2023-01-19T18:17:05.686202Z","shell.execute_reply":"2023-01-19T18:19:07.068654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install GPUtil\n\n# from GPUtil import showUtilization as gpu_usage\n# gpu_usage()    ","metadata":{"execution":{"iopub.status.busy":"2023-01-17T19:16:58.301370Z","iopub.execute_input":"2023-01-17T19:16:58.301999Z","iopub.status.idle":"2023-01-17T19:16:58.306196Z","shell.execute_reply.started":"2023-01-17T19:16:58.301964Z","shell.execute_reply":"2023-01-17T19:16:58.305229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#end","metadata":{"execution":{"iopub.status.busy":"2023-01-17T19:16:58.309402Z","iopub.execute_input":"2023-01-17T19:16:58.310017Z","iopub.status.idle":"2023-01-17T19:16:58.315348Z","shell.execute_reply.started":"2023-01-17T19:16:58.309983Z","shell.execute_reply":"2023-01-17T19:16:58.314349Z"},"trusted":true},"execution_count":null,"outputs":[]}]}