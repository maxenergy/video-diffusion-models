{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Video diffusion models","metadata":{}},{"cell_type":"code","source":"# !wget ..","metadata":{"execution":{"iopub.status.busy":"2023-01-20T20:04:39.551133Z","iopub.execute_input":"2023-01-20T20:04:39.552462Z","iopub.status.idle":"2023-01-20T20:04:39.576573Z","shell.execute_reply.started":"2023-01-20T20:04:39.552342Z","shell.execute_reply":"2023-01-20T20:04:39.575249Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!rm -rf ./dataset","metadata":{"execution":{"iopub.status.busy":"2023-01-20T20:04:40.746273Z","iopub.execute_input":"2023-01-20T20:04:40.747172Z","iopub.status.idle":"2023-01-20T20:04:42.047241Z","shell.execute_reply.started":"2023-01-20T20:04:40.747121Z","shell.execute_reply":"2023-01-20T20:04:42.045744Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!mkdir dataset\n!cp -R ../input/various-pokemon-image-dataset/ani/*.gif ./dataset\n!cp -R ../input/various-pokemon-image-dataset/ani/pokemon-ani/*.gif ./dataset\n!cp -R ../input/various-pokemon-image-dataset/ani-shiny/*.gif ./dataset\n# !ls ./dataset/*","metadata":{"execution":{"iopub.status.busy":"2023-01-20T20:04:42.053412Z","iopub.execute_input":"2023-01-20T20:04:42.054032Z","iopub.status.idle":"2023-01-20T20:05:08.227193Z","shell.execute_reply.started":"2023-01-20T20:04:42.053990Z","shell.execute_reply":"2023-01-20T20:05:08.225888Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Params\nimage_size = 64\nframes = 20\nprocess_batch_size = 256\n\nmax_run_minutes = 12 * 60 - 10","metadata":{"execution":{"iopub.status.busy":"2023-01-20T20:05:08.237661Z","iopub.execute_input":"2023-01-20T20:05:08.238404Z","iopub.status.idle":"2023-01-20T20:05:08.246645Z","shell.execute_reply.started":"2023-01-20T20:05:08.238366Z","shell.execute_reply":"2023-01-20T20:05:08.245611Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import time\nimport os\nstart_time = time.time()","metadata":{"execution":{"iopub.status.busy":"2023-01-20T20:05:08.250010Z","iopub.execute_input":"2023-01-20T20:05:08.250486Z","iopub.status.idle":"2023-01-20T20:05:08.260527Z","shell.execute_reply.started":"2023-01-20T20:05:08.250458Z","shell.execute_reply":"2023-01-20T20:05:08.259761Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Install dependencies","metadata":{}},{"cell_type":"code","source":"!pip install imagen_pytorch==1.18.11 --no-cache-dir","metadata":{"execution":{"iopub.status.busy":"2023-01-20T20:05:08.262443Z","iopub.execute_input":"2023-01-20T20:05:08.262855Z","iopub.status.idle":"2023-01-20T20:05:30.948524Z","shell.execute_reply.started":"2023-01-20T20:05:08.262815Z","shell.execute_reply":"2023-01-20T20:05:30.947346Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Collecting imagen_pytorch==1.18.11\n  Downloading imagen_pytorch-1.18.11-py3-none-any.whl (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting beartype\n  Downloading beartype-0.12.0-py3-none-any.whl (754 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m754.5/754.5 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.18.11) (4.64.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.18.11) (8.1.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.18.11) (1.21.6)\nCollecting einops-exts\n  Downloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.18.11) (4.20.1)\nCollecting einops>=0.6\n  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.18.11) (0.12.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.18.11) (2.1.0)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.18.11) (9.1.1)\nCollecting pytorch-warmup\n  Downloading pytorch_warmup-0.1.1-py3-none-any.whl (6.6 kB)\nRequirement already satisfied: pydantic in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.18.11) (1.8.2)\nCollecting ema-pytorch>=0.0.3\n  Downloading ema_pytorch-0.1.4-py3-none-any.whl (4.2 kB)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.18.11) (2022.11.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.18.11) (1.8.6)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.18.11) (0.1.97)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.18.11) (0.12.0)\nRequirement already satisfied: kornia in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.18.11) (0.5.8)\nRequirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.18.11) (1.11.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.18.11) (22.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->imagen_pytorch==1.18.11) (4.1.1)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from accelerate->imagen_pytorch==1.18.11) (6.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from accelerate->imagen_pytorch==1.18.11) (5.9.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->imagen_pytorch==1.18.11) (4.13.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets->imagen_pytorch==1.18.11) (5.0.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets->imagen_pytorch==1.18.11) (0.70.14)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets->imagen_pytorch==1.18.11) (0.10.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets->imagen_pytorch==1.18.11) (2.28.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets->imagen_pytorch==1.18.11) (3.2.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets->imagen_pytorch==1.18.11) (1.3.5)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets->imagen_pytorch==1.18.11) (0.18.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets->imagen_pytorch==1.18.11) (3.8.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets->imagen_pytorch==1.18.11) (0.3.6)\nRequirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning->imagen_pytorch==1.18.11) (0.11.0)\nRequirement already satisfied: lightning-utilities!=0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning->imagen_pytorch==1.18.11) (0.5.0)\nRequirement already satisfied: tensorboardX>=2.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning->imagen_pytorch==1.18.11) (2.5.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers->imagen_pytorch==1.18.11) (3.7.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers->imagen_pytorch==1.18.11) (0.12.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers->imagen_pytorch==1.18.11) (2021.11.10)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->imagen_pytorch==1.18.11) (0.13.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->imagen_pytorch==1.18.11) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->imagen_pytorch==1.18.11) (1.7.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->imagen_pytorch==1.18.11) (21.4.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->imagen_pytorch==1.18.11) (4.0.2)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->imagen_pytorch==1.18.11) (2.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->imagen_pytorch==1.18.11) (1.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->imagen_pytorch==1.18.11) (1.2.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->imagen_pytorch==1.18.11) (3.8.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets->imagen_pytorch==1.18.11) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets->imagen_pytorch==1.18.11) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets->imagen_pytorch==1.18.11) (1.26.13)\nCollecting protobuf<=3.20.1,>=3.8.0\n  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m122.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets->imagen_pytorch==1.18.11) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets->imagen_pytorch==1.18.11) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets->imagen_pytorch==1.18.11) (1.15.0)\nInstalling collected packages: protobuf, einops, beartype, pytorch-warmup, ema-pytorch, einops-exts, imagen_pytorch\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\ntfx-bsl 1.9.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.52.0 which is incompatible.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible.\ntensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible.\ntensorflow 2.6.4 requires typing-extensions<3.11,>=3.7, but you have typing-extensions 4.1.1 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\nortools 9.5.2237 requires protobuf>=4.21.5, but you have protobuf 3.20.1 which is incompatible.\nonnx 1.13.0 requires protobuf<4,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\nnnabla 1.32.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.20.1 which is incompatible.\ngoogle-api-core 1.33.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngcsfs 2022.5.0 requires fsspec==2022.5.0, but you have fsspec 2022.11.0 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed beartype-0.12.0 einops-0.6.0 einops-exts-0.0.4 ema-pytorch-0.1.4 imagen_pytorch-1.18.11 protobuf-3.20.1 pytorch-warmup-0.1.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## Utility functions to resize and crop GIFs","metadata":{}},{"cell_type":"code","source":"# GIF pre-processing\n\nimport numpy as np\nfrom torchvision import transforms as T\nfrom math import floor, fabs\nfrom PIL import Image, ImageSequence\n\n\nCHANNELS_TO_MODE = {\n    1 : 'L',\n    3 : 'RGB',\n    4 : 'RGBA'\n}\n\ndef center_crop(img, new_width, new_height): \n    width = img.size[0]\n    height = img.size[1]\n    left = int(np.ceil((width - new_width) / 2))\n    right = width - int(np.floor((width - new_width) / 2))\n    top = int(np.ceil((height - new_height) / 2))\n    bottom = height - int(np.floor((height - new_height) / 2))\n    return img.crop((left, top, right, bottom))\n\ndef resize_crop_img(img, width, height):\n    # width < height\n    if( img.size[0] < img.size[1]):\n        wpercent = (width/float(img.size[0]))\n        hsize = int((float(img.size[1])*float(wpercent)))\n        img = img.resize((width, hsize), Image.Resampling.LANCZOS)\n    else: # width >= height\n        hpercent = (height/float(img.size[1]))\n        wsize = int((float(img.size[0])*float(hpercent)))\n        img = img.resize((wsize, height), Image.Resampling.LANCZOS)\n    img = center_crop(img, width, height)\n    return img\n\ndef transform_gif(img, new_width, new_height, frames, channels = 3):\n    assert channels in CHANNELS_TO_MODE, f'channels {channels} invalid'\n    mode = CHANNELS_TO_MODE[channels]\n    gif_frames = img.n_frames\n    for i in range(0, frames):\n        img.seek(i % gif_frames)\n        if img.size[0] != new_width or img.size[1] != new_height:\n#             print(\"Resizing\")\n            img_out = resize_crop_img(img, new_width, new_height)\n        else:\n            img_out = img\n        yield img_out.convert(mode)\n        \n# tensor of shape (channels, frames, height, width) -> gif\ndef video_tensor_to_gif(tensor, path, fps = 10, loop = 0, optimize = True):\n    print(\"Converting video tensors to GIF\")\n    images = map(T.ToPILImage(), tensor.unbind(dim = 1))\n    first_img, *rest_imgs = images\n    print(1000/fps)\n    first_img.save(path, save_all = True, append_images = rest_imgs, duration = int(1000/fps), loop = loop, optimize = optimize)\n    print(\"Gif saved\")\n    return images\n\n# gif -> (channels, frame, height, width) tensor\ndef gif_to_tensor(path, width = 256, height = 256, frames = 32, channels = 3, transform = T.ToTensor()):\n#     print(f\"Converting {path} to video tensors\")\n    img = Image.open(path)\n    imgs = transform_gif(img, new_width = width, new_height = height, frames = frames, channels = channels)\n    tensors = tuple(map(transform, imgs))\n    return torch.stack(tensors, dim = 1)\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-01-20T20:05:30.952269Z","iopub.execute_input":"2023-01-20T20:05:30.952583Z","iopub.status.idle":"2023-01-20T20:05:32.876523Z","shell.execute_reply.started":"2023-01-20T20:05:30.952553Z","shell.execute_reply":"2023-01-20T20:05:32.875372Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Utility functions to process dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport shutil\nimport urllib\nimport traceback\n\nfrom concurrent.futures import ThreadPoolExecutor, wait\nimport time\nimport threading\nimport fnmatch\n\ndata_dir = \"./dataset\"\ndataset_files = fnmatch.filter(os.listdir(data_dir), '*.gif')\ndataset_size = len(dataset_files)\n\ncurrent_step = 0\nlist_videos = []\n\n\nexecutor = ThreadPoolExecutor(max_workers=15)\n\ndef process_parallel(file_img):\n    global list_videos\n#     print(f\"Processing image {file_img}\")\n    tensor = gif_to_tensor(file_img, width = image_size, height = image_size, frames = frames)\n    list_videos.append(tensor)\n    \ndef get_videos_parallel(index_start, index_end):\n    print(f\"Get videos {index_start} - {index_end}\")\n    global list_videos\n    list_videos = []\n    futures = []\n    for i in range(index_start, index_end):\n        if i >= dataset_size:\n            break;\n        future = executor.submit(process_parallel, os.path.join(data_dir, dataset_files[i]))\n        futures.append(future)\n    wait(futures)\n                \ndef get_next_videos():\n    global current_step\n    get_videos_parallel(current_step, current_step + process_batch_size)\n    current_step += len(list_videos)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-20T20:05:32.877865Z","iopub.execute_input":"2023-01-20T20:05:32.878777Z","iopub.status.idle":"2023-01-20T20:05:32.891710Z","shell.execute_reply.started":"2023-01-20T20:05:32.878731Z","shell.execute_reply":"2023-01-20T20:05:32.890732Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Utility functions to save and load checkpoints","metadata":{}},{"cell_type":"code","source":"import shutil\nimport torch\nimport time\nimport gc\nimport os\nfrom imagen_pytorch import Unet3D, ElucidatedImagen, ImagenTrainer\nfrom imagen_pytorch.data import Dataset\n\ncheckpoints_path = \"./\"\ncheckpoint_path = \"\"\n\n# If there is a checkpoint these changes automatically at runtime\nepoch_1 = 0\nepoch_2 = 0\n\nstep_1 = 0\nstep_2 = 0\n\ntrain_unet = 1\n\ndef save_checkpoint(trainer: ImagenTrainer):\n    global checkpoint_path\n    global current_step\n    global epoch_1\n    global epoch_2\n    global step_1\n    global step_2\n    print(\"Saving checkpoint\")\n    current_time = int(time.time())\n    if os.path.exists(checkpoint_path):\n        os.remove(checkpoint_path)\n    if train_unet == 1:\n        step_1 = current_step\n    else:\n        step_2 = current_step\n    checkpoint_path = os.path.join(checkpoints_path, f\"checkpoint-e1_{epoch_1}-s1_{step_1}-e2_{epoch_2}-s2_{step_2}-{current_time}.pt\")\n    trainer.save(checkpoint_path)\n\ndef update_config(checkpoint):\n    global epoch_1\n    global epoch_2\n    global step_1\n    global step_2\n    global train_unet\n    global current_step\n    splitted = (checkpoint.replace(\".pt\", \"\").split(\"checkpoint-\")[1]).split(\"-\")\n    epoch_1 = int(splitted[0].replace(\"e1_\", \"\"))\n    step_1 = int(splitted[1].replace(\"s1_\", \"\"))\n    epoch_2 = int(splitted[2].replace(\"e2_\", \"\"))\n    step_2 = int(splitted[3].replace(\"s2_\", \"\"))\n    print(\"Loaded configuration\")\n    print(f\"Epoch unet 1: {epoch_1}\")\n    print(f\"Steps unet 1: {step_1}\")\n    print(f\"Epoch unet 2: {epoch_2}\")\n    print(f\"Steps unet 2: {step_2}\")\n    if epoch_2 >= epoch_1:\n        train_unet = 1\n        if step_1 >= dataset_size:\n            current_step = 0\n            epoch_1 += 1\n        else:\n            current_step = step_1\n    else:\n        train_unet = 2\n        if step_2 >= dataset_size:\n            current_step = 0\n            epoch_2 += 1\n        else:\n            current_step = step_2\n    print(f\"Unet {train_unet} selected\")\n    print(f\"Current step: {current_step}\")\n    \ndef config_new_epoch():\n    global epoch_1\n    global epoch_2\n    global train_unet\n    global current_step\n    if train_unet == 1:\n        epoch_1 += 1\n    else:\n        epoch_2 += 1\n    current_step = 0\n        \ndef load_checkpoint(trainer: ImagenTrainer):\n    global checkpoint_path\n    global epoch_1\n    global epoch_2\n    global step_1\n    global step_2\n    global train_unet\n    global current_step\n    print(\"Loading checkpoint\")\n    timestamp = -1\n    for file in os.listdir(checkpoints_path):\n        if file.endswith('.pt'):\n            new_timestamp = int((file.split(\"-\")[-1]).replace(\".pt\", \"\"))\n            if new_timestamp > timestamp:\n                checkpoint_path = os.path.join(checkpoints_path, file)\n                timestamp = new_timestamp        \n    if not os.path.exists(checkpoint_path):\n        print(\"No checkpoint found -> starting from scratch\")\n        epoch_1 = 0\n        epoch_2 = 0\n        step_1 = 0\n        step_2 = 0\n        current_step = 0\n        train_unet = 1\n        return None\n    trainer.load(checkpoint_path)\n    update_config(checkpoint_path)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-01-20T20:05:32.896446Z","iopub.execute_input":"2023-01-20T20:05:32.896765Z","iopub.status.idle":"2023-01-20T20:05:35.276470Z","shell.execute_reply.started":"2023-01-20T20:05:32.896735Z","shell.execute_reply":"2023-01-20T20:05:35.275280Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/605 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbd56b82f7a7431f996f83a1acf385ff"}},"metadata":{}}]},{"cell_type":"code","source":"unet1 = Unet3D(\n    dim = 64,\n    cond_dim = 64,\n    dim_mults = (1, 2, 4, 8),\n)\n\nunet2 = Unet3D(\n    dim = 64,\n    cond_dim = 64,\n    dim_mults = (1, 2, 4, 8),\n)\n\nimagen = ElucidatedImagen(\n    unets = (unet1, unet2),\n    condition_on_text = False,\n    image_sizes = (16, 64),\n    random_crop_sizes = (None, 16),\n    num_sample_steps = 64,\n    cond_drop_prob = 0.2,                       # gives the probability of dropout for classifier-free guidance.\n    sigma_min = 0.002,                          # min noise level\n    sigma_max = (80, 160),                      # max noise level, double the max noise level for upsampler\n    sigma_data = 0.5,                           # standard deviation of data distribution\n    rho = 7,                                    # controls the sampling schedule\n    P_mean = -1.2,                              # mean of log-normal distribution from which noise is drawn for training\n    P_std = 1.2,                                # standard deviation of log-normal distribution from which noise is drawn for training\n    S_churn = 80,                               # parameters for stochastic sampling - depends on dataset, Table 5 in apper\n    S_tmin = 0.05,\n    S_tmax = 50,\n    S_noise = 1.003,\n).cuda()\n\ntrainer = ImagenTrainer(imagen)\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-01-20T20:05:35.278269Z","iopub.execute_input":"2023-01-20T20:05:35.279009Z","iopub.status.idle":"2023-01-20T20:05:41.373366Z","shell.execute_reply.started":"2023-01-20T20:05:35.278966Z","shell.execute_reply":"2023-01-20T20:05:41.372340Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train Unet","metadata":{}},{"cell_type":"code","source":"# Train \nload_checkpoint(trainer)\nwhile True:\n    # if execution time is more than max_run_minutes stops\n    if time.time() - start_time >= max_run_minutes * 60:\n        break\n    get_next_videos()\n    if len(list_videos) == 0:\n        save_checkpoint(trainer)\n        config_new_epoch()\n        get_next_videos()\n#         break\n    print(\"Generating tensor from videos\")\n    videos = torch.stack(list_videos, dim = 0).cuda()\n    print(f\"Training Unet {train_unet}\")\n    trainer(videos, unet_number = train_unet, max_batch_size = 32)\n    trainer.update(unet_number = train_unet)\n    del videos\nsave_checkpoint(trainer)","metadata":{"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2023-01-20T20:05:41.374835Z","iopub.execute_input":"2023-01-20T20:05:41.375218Z","iopub.status.idle":"2023-01-20T20:07:57.857483Z","shell.execute_reply.started":"2023-01-20T20:05:41.375163Z","shell.execute_reply":"2023-01-20T20:07:57.855992Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Loading checkpoint\ncheckpoint loaded from ./checkpoint-e1_3-s1_1513-e2_3-s2_1513-1674244981.pt\nLoaded configuration\nEpoch unet 1: 3\nSteps unet 1: 1513\nEpoch unet 2: 3\nSteps unet 2: 1513\nUnet 1 selected\nCurrent step: 0\nGet videos 0 - 256\nGenerating tensor from videos\nTraining Unet 1\nGet videos 256 - 512\nGenerating tensor from videos\nTraining Unet 1\nGet videos 512 - 768\nGenerating tensor from videos\nTraining Unet 1\nGet videos 768 - 1024\nGenerating tensor from videos\nTraining Unet 1\nGet videos 1024 - 1280\nGenerating tensor from videos\nTraining Unet 1\nGet videos 1280 - 1536\nGenerating tensor from videos\nTraining Unet 1\nGet videos 1513 - 1769\nSaving checkpoint\ncheckpoint saved to ./checkpoint-e1_4-s1_1513-e2_3-s2_1513-1674245244.pt\nGet videos 0 - 256\nGenerating tensor from videos\nTraining Unet 1\nGet videos 256 - 512\nGenerating tensor from videos\nTraining Unet 1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1949831905.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mvideos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_videos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training Unet {train_unet}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munet_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_unet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_unet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mvideos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/imagen_pytorch/trainer.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/imagen_pytorch/trainer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, unet_number, max_batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed_engine_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"## Sampling","metadata":{}},{"cell_type":"code","source":"load_checkpoint(trainer)\nvideos_out = trainer.sample(video_frames = 30)\nprint(videos_out.shape)\nvideo_tensor_to_gif(videos_out[0], f'out.gif', fps = 5)","metadata":{"execution":{"iopub.status.busy":"2023-01-20T20:07:57.859016Z","iopub.status.idle":"2023-01-20T20:07:57.859811Z","shell.execute_reply.started":"2023-01-20T20:07:57.859544Z","shell.execute_reply":"2023-01-20T20:07:57.859572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image\nImage(open('out.gif','rb').read())","metadata":{"execution":{"iopub.status.busy":"2023-01-20T20:07:57.861161Z","iopub.status.idle":"2023-01-20T20:07:57.861909Z","shell.execute_reply.started":"2023-01-20T20:07:57.861650Z","shell.execute_reply":"2023-01-20T20:07:57.861675Z"},"trusted":true},"execution_count":null,"outputs":[]}]}