{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/Vallasc/video-diffusion-pytorch\n!pip install ./video-diffusion-pytorch","metadata":{"execution":{"iopub.status.busy":"2023-02-05T19:56:39.950114Z","iopub.execute_input":"2023-02-05T19:56:39.951237Z","iopub.status.idle":"2023-02-05T19:56:56.306896Z","shell.execute_reply.started":"2023-02-05T19:56:39.950641Z","shell.execute_reply":"2023-02-05T19:56:56.305683Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'video-diffusion-pytorch'...\nremote: Enumerating objects: 420, done.\u001b[K\nremote: Counting objects: 100% (222/222), done.\u001b[K\nremote: Compressing objects: 100% (98/98), done.\u001b[K\nremote: Total 420 (delta 204), reused 139 (delta 124), pack-reused 198\u001b[K\nReceiving objects: 100% (420/420), 1.30 MiB | 2.92 MiB/s, done.\nResolving deltas: 100% (303/303), done.\nProcessing ./video-diffusion-pytorch\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting einops==0.4.0\n  Downloading einops-0.4.0-py3-none-any.whl (28 kB)\nCollecting einops-exts==0.0.3\n  Downloading einops_exts-0.0.3-py3-none-any.whl (3.8 kB)\nCollecting rotary-embedding-torch==0.1.5\n  Downloading rotary_embedding_torch-0.1.5-py3-none-any.whl (4.1 kB)\nRequirement already satisfied: torch==1.11 in /opt/conda/lib/python3.7/site-packages (from video-diffusion-pytorch==0.6.0) (1.11.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from video-diffusion-pytorch==0.6.0) (0.12.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from video-diffusion-pytorch==0.6.0) (4.64.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.11->video-diffusion-pytorch==0.6.0) (4.1.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->video-diffusion-pytorch==0.6.0) (2.28.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->video-diffusion-pytorch==0.6.0) (9.1.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->video-diffusion-pytorch==0.6.0) (1.21.6)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->video-diffusion-pytorch==0.6.0) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->video-diffusion-pytorch==0.6.0) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->video-diffusion-pytorch==0.6.0) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->video-diffusion-pytorch==0.6.0) (1.26.14)\nBuilding wheels for collected packages: video-diffusion-pytorch\n  Building wheel for video-diffusion-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for video-diffusion-pytorch: filename=video_diffusion_pytorch-0.6.0-py3-none-any.whl size=12732 sha256=1a3cd31eda29d394b04e3f5b6f9dfbfe38955f5ee672cf299bbe6e6aedaa1f7e\n  Stored in directory: /root/.cache/pip/wheels/f9/5b/86/c7401484e33c3961dcf349fbd0e64b02e27606816b9615d242\nSuccessfully built video-diffusion-pytorch\nInstalling collected packages: einops, einops-exts, rotary-embedding-torch, video-diffusion-pytorch\nSuccessfully installed einops-0.4.0 einops-exts-0.0.3 rotary-embedding-torch-0.1.5 video-diffusion-pytorch-0.6.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!mkdir dataset\n!cp -R ../input/various-pokemon-image-dataset/ani/*.gif ./dataset\n!cp -R ../input/various-pokemon-image-dataset/ani/pokemon-ani/*.gif ./dataset\n!cp -R ../input/various-pokemon-image-dataset/ani-shiny/*.gif ./dataset","metadata":{"execution":{"iopub.status.busy":"2023-02-05T19:56:56.311231Z","iopub.execute_input":"2023-02-05T19:56:56.311554Z","iopub.status.idle":"2023-02-05T19:57:30.800424Z","shell.execute_reply.started":"2023-02-05T19:56:56.311520Z","shell.execute_reply":"2023-02-05T19:57:30.799133Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom video_diffusion_pytorch import Unet3D, GaussianDiffusion, Trainer\n\ndevice = 'cuda'\n\nmodel = Unet3D (\n    dim = 64,\n    dim_mults = (1, 2, 4, 8),\n)\n\ndiffusion = GaussianDiffusion (\n    model,\n    image_size = 64,\n    num_frames = 17,\n    timesteps = 256,   # number of steps\n    loss_type = 'l1'    # L1 or L2\n).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-02-05T19:57:30.802050Z","iopub.execute_input":"2023-02-05T19:57:30.802442Z","iopub.status.idle":"2023-02-05T19:57:35.996427Z","shell.execute_reply.started":"2023-02-05T19:57:30.802390Z","shell.execute_reply":"2023-02-05T19:57:35.995407Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    diffusion,\n    './dataset',                         # this folder path needs to contain all your training data, as .gif files, of correct image size and number of frames\n    train_batch_size = 4,\n    train_lr = 3e-4,\n    save_and_sample_every = 1000,\n    train_num_steps = 700000,         # total training steps\n    gradient_accumulate_every = 2,    # gradient accumulation steps\n    ema_decay = 0.9999,               # exponential moving average decay\n    amp = True,                       # turn on mixed precision\n    use_path_as_cond = True,\n    results_folder = '.',\n    use_device = device\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-02-05T19:57:35.998878Z","iopub.execute_input":"2023-02-05T19:57:35.999538Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"found 1513 videos as gif files at ./dataset\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://github.com/huggingface/pytorch-transformers/archive/main.zip\" to /root/.cache/torch/hub/main.zip\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df932c2fb5b6433cadfc5807f1fcd2ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2164de1baef24aedb4916581cd2f1bd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a73b22659615434a93679aefd50be3b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87adf294a76944ed952c6d4fa576ef6f"}},"metadata":{}},{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8700bb14652f40c085434d95717baf52"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"0: 0.9150320887565613\n0: 0.9125497937202454\n1: 0.9167293906211853\n1: 0.875106930732727\n2: 0.8186975717544556\n2: 0.8200809359550476\n3: 0.7735721468925476\n3: 0.7629637718200684\n4: 0.7426764965057373\n4: 0.7499771118164062\n5: 0.7642797231674194\n5: 0.7386166453361511\n6: 0.72539883852005\n6: 0.7005208730697632\n7: 0.6726662516593933\n7: 0.6532084345817566\n8: 0.6407278776168823\n8: 0.5875948071479797\n9: 0.5460174083709717\n9: 0.6938575506210327\n10: 0.6738568544387817\n10: 0.634949266910553\n11: 0.6501777768135071\n11: 0.5715629458427429\n12: 0.4974050223827362\n12: 0.649747371673584\n13: 0.5389474630355835\n13: 0.5126618146896362\n14: 0.45347529649734497\n14: 0.46456319093704224\n15: 0.44061481952667236\n15: 0.4342651069164276\n16: 0.5976428985595703\n16: 0.36975032091140747\n17: 0.44796815514564514\n17: 0.4126411974430084\n18: 0.5395351052284241\n18: 0.3926975131034851\n19: 0.39091041684150696\n19: 0.38923975825309753\n20: 0.30438658595085144\n20: 0.3398861289024353\n21: 0.2868373692035675\n21: 0.32801303267478943\n22: 0.4225315451622009\n22: 0.3737694025039673\n23: 0.3875027894973755\n23: 0.26055029034614563\n24: 0.29777809977531433\n24: 0.3236457109451294\n25: 0.3398183286190033\n25: 0.37373945116996765\n26: 0.29941096901893616\n26: 0.4113087058067322\n27: 0.3212079107761383\n27: 0.24450059235095978\n28: 0.30418601632118225\n28: 0.28537487983703613\n29: 0.2990419566631317\n29: 0.2329109162092209\n30: 0.5145697593688965\n30: 0.2199946641921997\n31: 0.36056309938430786\n31: 0.2370130568742752\n32: 0.286755234003067\n32: 0.3432372808456421\n33: 0.2961078882217407\n33: 0.235507994890213\n34: 0.29414138197898865\n34: 0.314501017332077\n35: 0.3967570960521698\n35: 0.4186369478702545\n36: 0.33191704750061035\n36: 0.2300364077091217\n37: 0.2929394543170929\n37: 0.2567894458770752\n38: 0.34299740195274353\n38: 0.5076345801353455\n39: 0.37234535813331604\n39: 0.2653079926967621\n40: 0.41048067808151245\n40: 0.28478604555130005\n41: 0.3174980580806732\n41: 0.30172181129455566\n42: 0.2714924216270447\n42: 0.35598957538604736\n43: 0.31549257040023804\n43: 0.19608716666698456\n44: 0.20144394040107727\n44: 0.28160160779953003\n45: 0.18917331099510193\n45: 0.3122813105583191\n46: 0.3033607304096222\n46: 0.4942264258861542\n47: 0.32685962319374084\n47: 0.22452834248542786\n48: 0.17968368530273438\n48: 0.35846108198165894\n49: 0.30389541387557983\n49: 0.19683906435966492\n50: 0.20503464341163635\n50: 0.22557857632637024\n51: 0.18458706140518188\n51: 0.25588396191596985\n52: 0.3604801595211029\n52: 0.24806629121303558\n53: 0.18758727610111237\n53: 0.1834648847579956\n54: 0.23595023155212402\n54: 0.2139744907617569\n55: 0.3983939588069916\n55: 0.19258342683315277\n56: 0.22501082718372345\n56: 0.3275262415409088\n57: 0.3206803798675537\n57: 0.2601301670074463\n58: 0.265835165977478\n58: 0.2185029536485672\n59: 0.18660153448581696\n59: 0.16475419700145721\n60: 0.26331689953804016\n60: 0.24304765462875366\n61: 0.1675247997045517\n61: 0.2297855168581009\n62: 0.1778295934200287\n62: 0.17541323602199554\n63: 0.2379540055990219\n63: 0.17191120982170105\n64: 0.3140396177768707\n64: 0.2606562674045563\n65: 0.19084732234477997\n65: 0.26626595854759216\n66: 0.1772361844778061\n66: 0.1695319265127182\n67: 0.1564793586730957\n67: 0.305327832698822\n68: 0.13980282843112946\n68: 0.14333634078502655\n69: 0.31820186972618103\n69: 0.18338796496391296\n70: 0.13345591723918915\n70: 0.17620573937892914\n71: 0.12468215078115463\n71: 0.20955932140350342\n72: 0.3780148923397064\n72: 0.19432131946086884\n73: 0.17159387469291687\n73: 0.2893027663230896\n74: 0.24266090989112854\n74: 0.1845586895942688\n75: 0.17603881657123566\n75: 0.2317856401205063\n76: 0.28191351890563965\n76: 0.24335885047912598\n77: 0.172637939453125\n77: 0.23493559658527374\n78: 0.17513275146484375\n78: 0.2161499559879303\n79: 0.16617363691329956\n79: 0.2284066081047058\n80: 0.1527816653251648\n80: 0.17699207365512848\n81: 0.20883691310882568\n81: 0.2925492823123932\n82: 0.11952374130487442\n82: 0.28792575001716614\n83: 0.2797853648662567\n83: 0.2695520222187042\n84: 0.24703004956245422\n84: 0.17114634811878204\n85: 0.19500306248664856\n85: 0.15313053131103516\n86: 0.2483300268650055\n86: 0.2671886086463928\n87: 0.32517731189727783\n87: 0.2538677752017975\n88: 0.17677460610866547\n88: 0.18963347375392914\n89: 0.2366398125886917\n89: 0.2283511906862259\n90: 0.40076202154159546\n90: 0.13296177983283997\n91: 0.3701009750366211\n91: 0.28112897276878357\n92: 0.1691637635231018\n92: 0.31630098819732666\n93: 0.1601094752550125\n93: 0.3271614611148834\n94: 0.2907530963420868\n94: 0.204675555229187\n95: 0.2508532702922821\n95: 0.2117040604352951\n96: 0.306498259305954\n96: 0.28185853362083435\n97: 0.14279107749462128\n97: 0.1845681071281433\n98: 0.37180590629577637\n98: 0.13828864693641663\n99: 0.18524017930030823\n99: 0.21780186891555786\n100: 0.22760576009750366\n100: 0.13868960738182068\n101: 0.2019917219877243\n101: 0.25641676783561707\n102: 0.17309918999671936\n102: 0.22717225551605225\n103: 0.1654367297887802\n103: 0.16934116184711456\n104: 0.11462243646383286\n104: 0.19875550270080566\n105: 0.30394214391708374\n105: 0.2894956171512604\n106: 0.2126086801290512\n106: 0.25612759590148926\n107: 0.21238064765930176\n107: 0.18028351664543152\n108: 0.16823138296604156\n108: 0.16437186300754547\n109: 0.14845198392868042\n109: 0.2131873518228531\n110: 0.20826338231563568\n110: 0.2535240054130554\n111: 0.141159325838089\n111: 0.16292576491832733\n112: 0.20159919559955597\n112: 0.25144249200820923\n113: 0.2575201094150543\n113: 0.15532992780208588\n114: 0.2434261590242386\n114: 0.1487642377614975\n115: 0.17014287412166595\n115: 0.35101842880249023\n116: 0.20451322197914124\n116: 0.1698949933052063\n117: 0.1480623036623001\n117: 0.18822185695171356\n118: 0.20739316940307617\n118: 0.20074819028377533\n119: 0.18327696621418\n119: 0.2501733899116516\n120: 0.31001612544059753\n120: 0.2202267050743103\n121: 0.1560082733631134\n121: 0.2438158541917801\n122: 0.19971787929534912\n122: 0.3007654547691345\n123: 0.14265167713165283\n123: 0.2250555455684662\n124: 0.2765229344367981\n124: 0.1490146368741989\n125: 0.1283273696899414\n125: 0.15825869143009186\n126: 0.3705612123012543\n126: 0.26006844639778137\n127: 0.21556134521961212\n127: 0.19638799130916595\n128: 0.15012571215629578\n128: 0.14547407627105713\n129: 0.19162917137145996\n129: 0.15448415279388428\n130: 0.14452585577964783\n130: 0.17546015977859497\n131: 0.3061991035938263\n131: 0.2374911904335022\n132: 0.12065771967172623\n132: 0.14670927822589874\n133: 0.20409399271011353\n133: 0.27542680501937866\n134: 0.12166844308376312\n134: 0.18336015939712524\n135: 0.1358405351638794\n135: 0.20238558948040009\n136: 0.5029202103614807\n136: 0.11400509625673294\n137: 0.2123918980360031\n137: 0.25540637969970703\n138: 0.23698998987674713\n138: 0.22492781281471252\n139: 0.19443264603614807\n139: 0.20538672804832458\n140: 0.16388723254203796\n140: 0.1971106082201004\n141: 0.11337953805923462\n141: 0.1233220249414444\n142: 0.20640559494495392\n142: 0.20195980370044708\n143: 0.1365165263414383\n143: 0.16821035742759705\n144: 0.13450957834720612\n144: 0.18715743720531464\n145: 0.13573642075061798\n145: 0.2695225477218628\n146: 0.15645955502986908\n146: 0.14631563425064087\n147: 0.1322023719549179\n147: 0.18842701613903046\n148: 0.18930336833000183\n148: 0.25361862778663635\n149: 0.19558873772621155\n149: 0.11369691789150238\n150: 0.13213250041007996\n150: 0.2714390456676483\n151: 0.10485806316137314\n151: 0.2749480903148651\n152: 0.2692609429359436\n152: 0.3374767303466797\n153: 0.10750541090965271\n153: 0.2508941888809204\n154: 0.13696718215942383\n154: 0.199062317609787\n155: 0.16623114049434662\n155: 0.21865271031856537\n156: 0.17082709074020386\n156: 0.18563182651996613\n157: 0.22016705572605133\n157: 0.25227105617523193\n158: 0.13945062458515167\n158: 0.10281744599342346\n159: 0.17692342400550842\n159: 0.1543448120355606\n160: 0.12486691772937775\n160: 0.11541441828012466\n161: 0.19068078696727753\n161: 0.12074405699968338\n162: 0.12768331170082092\n162: 0.3106805682182312\n163: 0.19838258624076843\n163: 0.17766912281513214\n164: 0.19773860275745392\n164: 0.13855944573879242\n165: 0.12128282338380814\n165: 0.1400698870420456\n166: 0.1276993453502655\n166: 0.16635102033615112\n167: 0.26654917001724243\n167: 0.1401403844356537\n168: 0.1187460795044899\n168: 0.184440016746521\n169: 0.2850942313671112\n169: 0.15433356165885925\n170: 0.2664451599121094\n170: 0.1934577375650406\n171: 0.23930193483829498\n171: 0.27090439200401306\n172: 0.17508062720298767\n172: 0.12941274046897888\n173: 0.17157186567783356\n173: 0.13713636994361877\n174: 0.16947029531002045\n174: 0.24214988946914673\n175: 0.2261221706867218\n175: 0.21571609377861023\n176: 0.22034646570682526\n176: 0.14519178867340088\n177: 0.2536129653453827\n177: 0.19456887245178223\n178: 0.22992894053459167\n178: 0.13681301474571228\n179: 0.1128244549036026\n179: 0.3928033411502838\n180: 0.13522882759571075\n180: 0.17960838973522186\n181: 0.20385125279426575\n181: 0.20242898166179657\n182: 0.16263091564178467\n182: 0.15324848890304565\n183: 0.11541024595499039\n183: 0.12277229875326157\n184: 0.2387205958366394\n184: 0.1439025104045868\n185: 0.12986868619918823\n185: 0.1491088718175888\n186: 0.14822758734226227\n186: 0.20644992589950562\n187: 0.20033638179302216\n187: 0.2135951668024063\n188: 0.15654988586902618\n188: 0.2603103220462799\n189: 0.08243976533412933\n189: 0.14983496069908142\n190: 0.12887372076511383\n190: 0.20173391699790955\n191: 0.1944742649793625\n191: 0.25818365812301636\n192: 0.1390472948551178\n192: 0.12308657914400101\n193: 0.12199549376964569\n193: 0.19751055538654327\n194: 0.14002518355846405\n194: 0.2860947251319885\n195: 0.15131622552871704\n195: 0.4305209219455719\n196: 0.1212289109826088\n196: 0.20044457912445068\n197: 0.11180274188518524\n197: 0.14291074872016907\n198: 0.26157936453819275\n198: 0.1340538114309311\n199: 0.24603570997714996\n199: 0.20923955738544464\n200: 0.1408618539571762\n200: 0.27701884508132935\n201: 0.16083478927612305\n201: 0.1172403022646904\n202: 0.11726800352334976\n202: 0.17746619880199432\n203: 0.2393624633550644\n203: 0.186021089553833\n204: 0.19253630936145782\n204: 0.3203105926513672\n205: 0.24400240182876587\n205: 0.2106371372938156\n206: 0.1966092735528946\n206: 0.24779418110847473\n207: 0.20230738818645477\n207: 0.15374021232128143\n208: 0.2522065341472626\n208: 0.165205717086792\n209: 0.13675035536289215\n209: 0.10870910435914993\n210: 0.2965143024921417\n210: 0.17424491047859192\n211: 0.18991854786872864\n211: 0.1596376746892929\n212: 0.2761490046977997\n212: 0.19298918545246124\n213: 0.09665732830762863\n213: 0.10815529525279999\n214: 0.15026088058948517\n214: 0.13137206435203552\n215: 0.16746562719345093\n215: 0.18029311299324036\n216: 0.30341145396232605\n216: 0.27310842275619507\n217: 0.13068099319934845\n217: 0.1933773308992386\n218: 0.15123189985752106\n218: 0.14192943274974823\n219: 0.11342013627290726\n219: 0.17314070463180542\n220: 0.13075079023838043\n220: 0.1269831359386444\n221: 0.12530933320522308\n221: 0.23884306848049164\n222: 0.28289586305618286\n222: 0.26075178384780884\n223: 0.1639396697282791\n223: 0.19354312121868134\n224: 0.11865178495645523\n224: 0.20388004183769226\n225: 0.12356814742088318\n225: 0.13944457471370697\n226: 0.11675724387168884\n226: 0.31151723861694336\n227: 0.2020190805196762\n227: 0.09614166617393494\n228: 0.2589225172996521\n228: 0.2001488357782364\n229: 0.1305191069841385\n229: 0.11930828541517258\n230: 0.21331727504730225\n230: 0.1490011215209961\n231: 0.10840309411287308\n231: 0.12583743035793304\n232: 0.12345389276742935\n232: 0.25320249795913696\n233: 0.121888168156147\n233: 0.1844528466463089\n234: 0.09192472696304321\n234: 0.2044646441936493\n235: 0.13903766870498657\n235: 0.2621454894542694\n236: 0.11689220368862152\n236: 0.1791801005601883\n237: 0.09879092127084732\n237: 0.10820500552654266\n238: 0.17043457925319672\n238: 0.14970943331718445\n239: 0.1787102371454239\n239: 0.21183907985687256\n240: 0.11414947360754013\n240: 0.14822648465633392\n241: 0.1275739073753357\n241: 0.23087409138679504\n242: 0.16271845996379852\n242: 0.21028625965118408\n243: 0.32996666431427\n243: 0.14676441252231598\n244: 0.10664985328912735\n244: 0.33871954679489136\n245: 0.10969743877649307\n245: 0.18189162015914917\n246: 0.10932546854019165\n246: 0.11408521234989166\n247: 0.2039058655500412\n247: 0.11803457140922546\n248: 0.1087043359875679\n248: 0.2773449420928955\n249: 0.11870674788951874\n249: 0.17795786261558533\n250: 0.15156100690364838\n250: 0.2836712598800659\n251: 0.13711592555046082\n251: 0.11755190044641495\n252: 0.13559390604496002\n252: 0.1011618971824646\n253: 0.1237829253077507\n253: 0.1372588574886322\n254: 0.16133278608322144\n254: 0.1330983191728592\n255: 0.14040876924991608\n255: 0.1944945901632309\n256: 0.2762342393398285\n256: 0.13918942213058472\n257: 0.1901586651802063\n257: 0.11393070966005325\n258: 0.2217203825712204\n258: 0.13342374563217163\n259: 0.22022564709186554\n259: 0.14802202582359314\n260: 0.1936400979757309\n260: 0.25890952348709106\n261: 0.13991400599479675\n261: 0.1714463084936142\n262: 0.10748142004013062\n262: 0.3206702470779419\n263: 0.10177335888147354\n263: 0.19720523059368134\n264: 0.16408637166023254\n264: 0.18339267373085022\n265: 0.1526588797569275\n265: 0.13087880611419678\n266: 0.2505183517932892\n266: 0.16286849975585938\n267: 0.08699991554021835\n267: 0.1853151172399521\n268: 0.12679947912693024\n268: 0.13139963150024414\n269: 0.13186773657798767\n269: 0.1415989100933075\n270: 0.19705085456371307\n270: 0.18109123408794403\n271: 0.15765464305877686\n271: 0.08999935537576675\n272: 0.14119555056095123\n272: 0.13182534277439117\n273: 0.3141879439353943\n273: 0.15257792174816132\n274: 0.19020429253578186\n274: 0.16354860365390778\n275: 0.08810295909643173\n275: 0.12822818756103516\n276: 0.29369381070137024\n276: 0.23415623605251312\n277: 0.16678905487060547\n277: 0.15712440013885498\n278: 0.11095111072063446\n278: 0.11146891862154007\n279: 0.18217705190181732\n279: 0.11239159107208252\n280: 0.18949277698993683\n280: 0.13842207193374634\n281: 0.10990511626005173\n281: 0.1415361613035202\n282: 0.16581116616725922\n282: 0.1195359006524086\n283: 0.16575084626674652\n283: 0.13464215397834778\n284: 0.11533398926258087\n284: 0.13964799046516418\n285: 0.18586808443069458\n285: 0.16942577064037323\n286: 0.19896826148033142\n286: 0.15381170809268951\n287: 0.10929981619119644\n287: 0.12824979424476624\n288: 0.1459413319826126\n288: 0.10412391275167465\n289: 0.14974842965602875\n289: 0.14449633657932281\n290: 0.2887375056743622\n290: 0.10337699204683304\n291: 0.22270461916923523\n291: 0.19102337956428528\n292: 0.2213234305381775\n292: 0.15984603762626648\n293: 0.1298958659172058\n293: 0.1568034589290619\n294: 0.1789395958185196\n294: 0.15122707188129425\n295: 0.23036545515060425\n295: 0.20720449090003967\n296: 0.11457563936710358\n296: 0.16958580911159515\n297: 0.18224595487117767\n297: 0.13307784497737885\n","output_type":"stream"}]},{"cell_type":"code","source":"# from IPython.display import Image\n# Image(open('1.gif','rb').read())","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}