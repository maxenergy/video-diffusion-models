{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !wget","metadata":{"execution":{"iopub.status.busy":"2023-02-06T14:18:35.464542Z","iopub.execute_input":"2023-02-06T14:18:35.465965Z","iopub.status.idle":"2023-02-06T14:18:35.486470Z","shell.execute_reply.started":"2023-02-06T14:18:35.465279Z","shell.execute_reply":"2023-02-06T14:18:35.485518Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/Vallasc/video-diffusion-pytorch\n!pip install ./video-diffusion-pytorch","metadata":{"execution":{"iopub.status.busy":"2023-02-06T14:18:35.488301Z","iopub.execute_input":"2023-02-06T14:18:35.488634Z","iopub.status.idle":"2023-02-06T14:18:50.904252Z","shell.execute_reply.started":"2023-02-06T14:18:35.488601Z","shell.execute_reply":"2023-02-06T14:18:50.903062Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'video-diffusion-pytorch'...\nremote: Enumerating objects: 436, done.\u001b[K\nremote: Counting objects: 100% (238/238), done.\u001b[K\nremote: Compressing objects: 100% (108/108), done.\u001b[K\nremote: Total 436 (delta 216), reused 149 (delta 130), pack-reused 198\u001b[K\nReceiving objects: 100% (436/436), 1.30 MiB | 17.31 MiB/s, done.\nResolving deltas: 100% (315/315), done.\nProcessing ./video-diffusion-pytorch\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting einops==0.4.0\n  Downloading einops-0.4.0-py3-none-any.whl (28 kB)\nCollecting einops-exts==0.0.3\n  Downloading einops_exts-0.0.3-py3-none-any.whl (3.8 kB)\nCollecting rotary-embedding-torch==0.1.5\n  Downloading rotary_embedding_torch-0.1.5-py3-none-any.whl (4.1 kB)\nRequirement already satisfied: torch==1.11 in /opt/conda/lib/python3.7/site-packages (from video-diffusion-pytorch==0.6.0) (1.11.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from video-diffusion-pytorch==0.6.0) (0.12.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from video-diffusion-pytorch==0.6.0) (4.64.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.11->video-diffusion-pytorch==0.6.0) (4.1.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->video-diffusion-pytorch==0.6.0) (9.1.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->video-diffusion-pytorch==0.6.0) (1.21.6)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->video-diffusion-pytorch==0.6.0) (2.28.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->video-diffusion-pytorch==0.6.0) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->video-diffusion-pytorch==0.6.0) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->video-diffusion-pytorch==0.6.0) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->video-diffusion-pytorch==0.6.0) (2022.12.7)\nBuilding wheels for collected packages: video-diffusion-pytorch\n  Building wheel for video-diffusion-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for video-diffusion-pytorch: filename=video_diffusion_pytorch-0.6.0-py3-none-any.whl size=12859 sha256=1aa7e61876b61cc30bbd000b050849855f8eb75aa487f5060761410bb672c6e1\n  Stored in directory: /root/.cache/pip/wheels/f9/5b/86/c7401484e33c3961dcf349fbd0e64b02e27606816b9615d242\nSuccessfully built video-diffusion-pytorch\nInstalling collected packages: einops, einops-exts, rotary-embedding-torch, video-diffusion-pytorch\nSuccessfully installed einops-0.4.0 einops-exts-0.0.3 rotary-embedding-torch-0.1.5 video-diffusion-pytorch-0.6.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!mkdir dataset\n!cp -R ../input/fireworks-gif/*.gif ./dataset","metadata":{"execution":{"iopub.status.busy":"2023-02-06T14:18:50.905981Z","iopub.execute_input":"2023-02-06T14:18:50.906382Z","iopub.status.idle":"2023-02-06T14:19:18.440400Z","shell.execute_reply.started":"2023-02-06T14:18:50.906315Z","shell.execute_reply":"2023-02-06T14:19:18.438981Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom video_diffusion_pytorch import Unet3D, GaussianDiffusion, Trainer\n\ndevice = 'cuda'\n\nmodel = Unet3D (\n    dim = 64,\n    dim_mults = (1, 2, 4, 8),\n)\n\ndiffusion = GaussianDiffusion (\n    model,\n    image_size = 64,\n    num_frames = 17,\n    timesteps = 256,   # number of steps\n    loss_type = 'l1'    # L1 or L2\n).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-02-06T14:19:18.446478Z","iopub.execute_input":"2023-02-06T14:19:18.449084Z","iopub.status.idle":"2023-02-06T14:19:23.723200Z","shell.execute_reply.started":"2023-02-06T14:19:18.449035Z","shell.execute_reply":"2023-02-06T14:19:23.722187Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    diffusion,\n    './dataset',                      # this folder path needs to contain all your training data, as .gif files, of correct image size and number of frames\n    train_batch_size = 4,\n    train_lr = 3e-4,\n    save_and_sample_every = 1000,\n    train_num_steps = 1000000,        # total training steps\n    gradient_accumulate_every = 2,    # gradient accumulation steps\n    ema_decay = 0.9999,               # exponential moving average decay\n    amp = True,                       # turn on mixed precision\n    results_folder = '.',\n    use_device = device\n)\ntrainer.load()\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-02-06T14:19:23.724516Z","iopub.execute_input":"2023-02-06T14:19:23.725088Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"found 3027 videos as gif files at ./dataset\nneed to have at least one milestone to load from latest checkpoint!\n0: 0.9263491630554199\n0: 0.9204065799713135\n1: 0.9068408608436584\n1: 0.9564096331596375\n2: 0.8232550621032715\n2: 0.8201581239700317\n","output_type":"stream"},{"name":"stderr","text":"sampling loop time step:  20%|█▉        | 51/256 [00:28<01:52,  1.82it/s]","output_type":"stream"}]},{"cell_type":"code","source":"# !ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import Image\n# Image(open('1.gif','rb').read())","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}