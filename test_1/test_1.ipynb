{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Video diffusion models","metadata":{}},{"cell_type":"code","source":"# Here wget old checkpoint\n!wget https://www.kaggleusercontent.com/kf/116028501/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..yiQygq7QLyl2hi3wXNmCVw.CMuGLSu-255AgqUB-b03zwv1WyfgMMSGEO6GpkhFnC8Q4NjeZGIzAk9pmKztWHLKZZN8fK_XjcKhOOJr1YXbPzA_Rt8B-AQ5INCTR1-GUE-jBKl7q0N2mcNe8Z6BN5REYZV5qR40Z3kiZP6WTTadksd6cOKcwzAc2qx2MyIwoyP9yVUppEVS03yck4k50BSNJsuPfdiTz28H1UC1JpCUg5PuJzBiysYzNomY_l4tkcJFyrHLUfgXEy9ZpQgWpW4eq1CB4YMTFap7x5nAevMLEwQdpqmGFMpcwf4cDBR_GSzR2Id__EsefEEquyXY0BBeEH4Qm3_hFE1JO6G19IHGtkqCRx5u88ba2HGVbiKFjy_FIRcTLHLlDAfuSbIr0V70JQ6BuIIb5WfkJC7cCOlMpmout43wOxqK32lhV6vnfZH4m_TefJF3pWKnMSTAfwoBZJYq0HGt9qb8sRI41fUTRiJZiRNsVnSMWsVJEt13ohx2ojKwiwVHEZW4WTqbQLd1LCvbU0ObLGF4eJL2KoDBkKS9B9i2V9593I8Hjts9smTf1v8rl6-I1oEiICYwlBhx9RNqTIiVlkmOXXvSDXGUBMl_onGQzL5msHhijN9cpPjOZrstJQQ0K-d29FRkkqkr2khRbS-LmG6BR5R2a9eHxQ.Yuuy3lCvzbgaG78Iq1j_yQ/checkpoint-unet_1-epoch_24-step_125782-1673397908.pt","metadata":{"execution":{"iopub.status.busy":"2022-12-23T12:46:06.139288Z","iopub.execute_input":"2022-12-23T12:46:06.139694Z","iopub.status.idle":"2022-12-23T12:47:02.249617Z","shell.execute_reply.started":"2022-12-23T12:46:06.139656Z","shell.execute_reply":"2022-12-23T12:47:02.248409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Params\nimage_size = 128\nframes = 12\ndownload_batch_size = 128\ndownload_workers = 20\n\ndataset_size = 125782\n\n# If there is a checkpoint these changes automatically at runtime\nepoch = 1\ntrain_unet = 1\ncurrent_step = 0","metadata":{"execution":{"iopub.status.busy":"2022-12-23T12:47:02.252764Z","iopub.execute_input":"2022-12-23T12:47:02.253129Z","iopub.status.idle":"2022-12-23T12:47:02.258756Z","shell.execute_reply.started":"2022-12-23T12:47:02.253097Z","shell.execute_reply":"2022-12-23T12:47:02.257590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport os\nstart_time = time.time()\n\n# Clean tmp files\nfor file in os.listdir(\"./\"):\n    if file.endswith('.gif'):\n        os.remove(file)","metadata":{"execution":{"iopub.status.busy":"2022-12-23T12:47:02.262402Z","iopub.execute_input":"2022-12-23T12:47:02.262832Z","iopub.status.idle":"2022-12-23T12:47:02.269455Z","shell.execute_reply.started":"2022-12-23T12:47:02.262795Z","shell.execute_reply":"2022-12-23T12:47:02.268314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Install dependencies","metadata":{}},{"cell_type":"code","source":"!pip install imagen_pytorch==1.16.5 --no-cache-dir","metadata":{"execution":{"iopub.status.busy":"2022-12-23T12:47:02.271467Z","iopub.execute_input":"2022-12-23T12:47:02.271835Z","iopub.status.idle":"2022-12-23T12:47:16.921887Z","shell.execute_reply.started":"2022-12-23T12:47:02.271799Z","shell.execute_reply":"2022-12-23T12:47:16.920022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utility functions to resize and crop GIFs","metadata":{}},{"cell_type":"code","source":"# GIF pre-processing\n\nimport numpy as np\nfrom torchvision import transforms as T\nfrom math import floor, fabs\nfrom PIL import Image, ImageSequence\n\n\nCHANNELS_TO_MODE = {\n    1 : 'L',\n    3 : 'RGB',\n    4 : 'RGBA'\n}\n\ndef center_crop(img, new_width, new_height): \n    width = img.size[0]\n    height = img.size[1]\n    left = int(np.ceil((width - new_width) / 2))\n    right = width - int(np.floor((width - new_width) / 2))\n    top = int(np.ceil((height - new_height) / 2))\n    bottom = height - int(np.floor((height - new_height) / 2))\n    return img.crop((left, top, right, bottom))\n\ndef resize_crop_img(img, width, height):\n    # width < height\n    if( img.size[0] < img.size[1]):\n        wpercent = (width/float(img.size[0]))\n        hsize = int((float(img.size[1])*float(wpercent)))\n        img = img.resize((width, hsize), Image.Resampling.LANCZOS)\n    else: # width >= height\n        hpercent = (height/float(img.size[1]))\n        wsize = int((float(img.size[0])*float(hpercent)))\n        img = img.resize((wsize, height), Image.Resampling.LANCZOS)\n    img = center_crop(img, width, height)\n    # print(img.size[0])\n    # print(img.size[1])\n    return img\n\ndef transform_gif(img, new_width, new_height, frames, channels = 3):\n    assert channels in CHANNELS_TO_MODE, f'channels {channels} invalid'\n    mode = CHANNELS_TO_MODE[channels]\n    gif_frames = img.n_frames\n    for i in range(0, frames):\n        img.seek(i % gif_frames)\n        img_out = resize_crop_img(img, new_width, new_height)\n        yield img_out.convert(mode)\n        \n# tensor of shape (channels, frames, height, width) -> gif\ndef video_tensor_to_gif(tensor, path, fps = 10, loop = 0, optimize = True):\n    print(\"Converting video tensors to GIF\")\n    images = map(T.ToPILImage(), tensor.unbind(dim = 1))\n    first_img, *rest_imgs = images\n    print(1000/fps)\n    first_img.save(path, save_all = True, append_images = rest_imgs, duration = int(1000/fps), loop = loop, optimize = optimize)\n    print(\"Gif saved\")\n    return images\n\n# gif -> (channels, frame, height, width) tensor\ndef gif_to_tensor(path, width = 256, height = 256, frames = 32, channels = 3, transform = T.ToTensor()):\n    print(\"Converting GIF to video tensors\")\n    img = Image.open(path)\n    imgs = transform_gif(img, new_width = width, new_height = height, frames = frames, channels = channels)\n    tensors = tuple(map(transform, imgs))\n    return torch.stack(tensors, dim = 1)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-12-23T12:47:16.924760Z","iopub.execute_input":"2022-12-23T12:47:16.925980Z","iopub.status.idle":"2022-12-23T12:47:18.925214Z","shell.execute_reply.started":"2022-12-23T12:47:16.925930Z","shell.execute_reply":"2022-12-23T12:47:18.924186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utility functions to download dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport shutil\nimport urllib\n\nfrom concurrent.futures import ThreadPoolExecutor, wait\nimport time\nimport threading\n\ntrain_url = \"https://raw.githubusercontent.com/raingo/TGIF-Release/master/data/tgif-v1.0.tsv\"\ntrain_data = \"./train_data.tvs\"\n\ncurrent_step = 0\ntexts = []\nlist_videos = []\n\n\ndef download_url(url, root, filename=None):\n    \"\"\"Download a file from a url and place it in root.\n    Args:\n        url (str): URL to download file from\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n    \"\"\"\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = os.path.basename(url)\n    fpath = os.path.join(root, filename)\n\n    os.makedirs(root, exist_ok=True)\n\n    try:\n        #print('Downloading ' + url + ' to ' + fpath)\n        urllib.request.urlretrieve(url, fpath)\n    except (urllib.error.URLError, IOError) as e:\n        if url[:5] == 'https':\n            url = url.replace('https:', 'http:')\n            print('Failed download. Trying https -> http instead.'\n                    ' Downloading ' + url + ' to ' + fpath)\n            urllib.request.urlretrieve(url, fpath)\n\ndef get_videos(index_start, index_end):\n    global texts\n    global list_videos\n    \n    texts = []\n    list_videos = []\n\n    with open(\"train_data.tvs\") as fp:\n        for i, line in enumerate(fp):\n            if i >= index_start and i< index_end :\n                try:\n                    file_img, file_text = line.split(\"\\t\")\n                    print(f\"Downloading image {i}\")\n                    download_url(file_img, \"./\", \"download.gif\")\n                    tensor = gif_to_tensor('download.gif', width = image_size, height = image_size, frames = frames)\n                    list_videos.append(tensor)\n                    file_text = file_text[:-1] # Remove \\n\n                    texts.append(file_text)\n                    os.remove('download.gif')\n                except Exception as ex:\n                    print(ex)\n                    pass\n            elif i > index_end:\n                break\n\nlock = threading.Lock()\nexecutor = ThreadPoolExecutor(max_workers=download_workers)\n\ndef download_process_parallel(index, file_img, file_text):\n    try:\n        print(f\"Downloading image {index}\")\n        download_url(file_img, \"./\", f\"{index}.gif\")\n        tensor = gif_to_tensor(f\"{index}.gif\", width = image_size, height = image_size, frames = frames)\n        file_text = file_text[:-1] # Remove \\n\n        with lock:\n            list_videos.append(tensor)\n            texts.append(file_text)\n        os.remove(f\"{index}.gif\")\n    except Exception as ex:\n        print(ex)\n        pass\n    \ndef get_videos_parallel(index_start, index_end):\n    global texts\n    global list_videos\n    \n    texts = []\n    list_videos = []\n\n    with open(\"train_data.tvs\") as fp:\n        futures = []\n        for i, line in enumerate(fp):\n            if i >= index_start and i< index_end :\n                file_img, file_text = line.split(\"\\t\")\n                future = executor.submit(download_process_parallel, i, file_img, file_text)\n                futures.append(future)\n            elif i > index_end:\n                break\n        wait(futures)\n                \ndef get_next_videos():\n    global current_step\n    get_videos_parallel(current_step, current_step + download_batch_size)\n    current_step += len(texts)\n\n# Download train data file\nif not os.path.exists(train_data):\n    download_url(train_url, \"./\", train_data)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-12-23T12:47:18.926774Z","iopub.execute_input":"2022-12-23T12:47:18.927387Z","iopub.status.idle":"2022-12-23T12:47:20.064331Z","shell.execute_reply.started":"2022-12-23T12:47:18.927348Z","shell.execute_reply":"2022-12-23T12:47:20.062531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utility functions to save and load checkpoints","metadata":{}},{"cell_type":"code","source":"import shutil\nimport torch\nimport time\nimport gc\nimport os\nfrom imagen_pytorch import ImagenTrainer\nfrom imagen_pytorch.data import Dataset\n\ncheckpoints_path = \"./\"\ncheckpoint_path = \"\"\n\ndef save_checkpoint(trainer: ImagenTrainer):\n    global checkpoint_path\n    print(\"Saving checkpoint\")\n    current_time = int(time.time())\n    if os.path.exists(checkpoint_path):\n        os.remove(checkpoint_path)\n    checkpoint_path = os.path.join(checkpoints_path, f\"checkpoint-unet_{train_unet}-epoch_{epoch}-step_{current_step}-{current_time}.pt\")\n    trainer.save(checkpoint_path)\n\ndef update_config(checkpoint):\n    global epoch\n    global train_unet\n    global current_step\n    splitted = (checkpoint.replace(\".pt\", \"\").split(\"checkpoint-\")[1]).split(\"-\")\n    train_unet = int(splitted[0].replace(\"unet_\", \"\"))\n    epoch = int(splitted[1].replace(\"epoch_\", \"\"))\n    current_step = int(splitted[2].replace(\"step_\", \"\"))\n    print(\"Loaded configuration\")\n    print(f\"Step: {current_step}\")\n    print(f\"Epoch: {epoch}\")\n    print(f\"Unet: {train_unet}\")\n    if current_step >= dataset_size:\n        if train_unet == 2:\n            print(\"End of training Unet 2 -> New epoch\")\n            train_unet = 1\n            epoch += 1\n        else:\n            print(\"End of training Unet 1\")\n            train_unet = 2\n        current_step = 0\n        print(\"New configuration\")\n        print(f\"Step: {current_step}\")\n        print(f\"Epoch: {epoch}\")\n        print(f\"Unet: {train_unet}\")\n        \ndef load_checkpoint(trainer: ImagenTrainer):\n    global checkpoint_path\n    print(\"Loading checkpoint\")\n    timestamp = -1\n    for file in os.listdir(checkpoints_path):\n        if file.endswith('.pt'):\n            new_timestamp = int((file.split(\"-\")[4]).replace(\".pt\", \"\"))\n            if new_timestamp > timestamp:\n                checkpoint_path = os.path.join(checkpoints_path, file)\n                timestamp = new_timestamp        \n    if not os.path.exists(checkpoint_path):\n        print(\"No checkpoint found -> starting from scratch\")\n        return None\n    trainer.load(checkpoint_path)\n    update_config(checkpoint_path)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-12-23T12:47:20.066175Z","iopub.execute_input":"2022-12-23T12:47:20.066620Z","iopub.status.idle":"2022-12-23T12:47:22.918605Z","shell.execute_reply.started":"2022-12-23T12:47:20.066579Z","shell.execute_reply":"2022-12-23T12:47:22.917490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imagen_pytorch import Unet3D, ElucidatedImagen, ImagenTrainer\n\nunet1 = Unet3D(\n    dim = 64,\n    cond_dim = 128,\n    dim_mults = (1, 2, 4, 8),\n    num_resnet_blocks = 3,\n    layer_attns = (False, True, True, True),\n    layer_cross_attns = (False, True, True, True)\n)\n\nunet2 = Unet3D(\n    dim = 64,\n    cond_dim = 128,\n    dim_mults = (1, 2, 4, 8),\n    num_resnet_blocks = (2, 4, 8, 8),\n    layer_attns = (False, False, False, True),\n    layer_cross_attns = (False, False, False, True)\n)\n\nimagen = ElucidatedImagen(\n    unets = (unet1, unet2),\n    image_sizes = (16, 64),\n    random_crop_sizes = (None, 16),\n    num_sample_steps = 64,\n    cond_drop_prob = 0.1,                       # gives the probability of dropout for classifier-free guidance.\n    sigma_min = 0.002,                          # min noise level\n    sigma_max = (80, 160),                      # max noise level, double the max noise level for upsampler\n    sigma_data = 0.5,                           # standard deviation of data distribution\n    rho = 7,                                    # controls the sampling schedule\n    P_mean = -1.2,                              # mean of log-normal distribution from which noise is drawn for training\n    P_std = 1.2,                                # standard deviation of log-normal distribution from which noise is drawn for training\n    S_churn = 80,                               # parameters for stochastic sampling - depends on dataset, Table 5 in apper\n    S_tmin = 0.05,\n    S_tmax = 50,\n    S_noise = 1.003,\n).cuda()\n\ntrainer = ImagenTrainer(imagen)\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-12-23T12:47:22.920544Z","iopub.execute_input":"2022-12-23T12:47:22.921286Z","iopub.status.idle":"2022-12-23T12:47:30.090499Z","shell.execute_reply.started":"2022-12-23T12:47:22.921244Z","shell.execute_reply":"2022-12-23T12:47:30.089478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Unet","metadata":{}},{"cell_type":"code","source":"# Train \nload_checkpoint(trainer)\n# counter = 0\nwhile True:\n    # if execution time is more than 11h40m stops\n    if time.time() - start_time > 42000:\n        break\n    get_next_videos()\n    # if there are no more videos stops\n    if len(texts) == 0:\n        break\n    print(\"Generating tensor from videos\")\n    videos = torch.stack(list_videos, dim = 0).cuda()\n    print(f\"Training Unet {train_unet}\")\n    trainer(videos, texts = texts, unet_number = train_unet, max_batch_size = 32)\n    trainer.update(unet_number = train_unet)\n    del videos\n#     if counter % 100 == 0:\n#         break\nsave_checkpoint(trainer)","metadata":{"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2022-12-23T12:47:30.093750Z","iopub.execute_input":"2022-12-23T12:47:30.094542Z","iopub.status.idle":"2022-12-23T12:47:39.342529Z","shell.execute_reply.started":"2022-12-23T12:47:30.094487Z","shell.execute_reply":"2022-12-23T12:47:39.341149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts_sample = ['red cat']\nload_checkpoint(trainer)\nvideos_out = trainer.sample(texts = texts_sample, video_frames = 20)\nprint(videos_out.shape)\nvideo_tensor_to_gif(videos_out[0], f'out.gif', fps = 5)","metadata":{"execution":{"iopub.status.busy":"2022-12-23T12:47:39.345535Z","iopub.execute_input":"2022-12-23T12:47:39.345832Z","iopub.status.idle":"2022-12-23T12:49:26.391153Z","shell.execute_reply.started":"2022-12-23T12:47:39.345804Z","shell.execute_reply":"2022-12-23T12:49:26.390027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install GPUtil\n\n# from GPUtil import showUtilization as gpu_usage\n# gpu_usage()    ","metadata":{"execution":{"iopub.status.busy":"2022-12-23T12:49:26.392711Z","iopub.execute_input":"2022-12-23T12:49:26.393382Z","iopub.status.idle":"2022-12-23T12:49:26.397897Z","shell.execute_reply.started":"2022-12-23T12:49:26.393341Z","shell.execute_reply":"2022-12-23T12:49:26.396778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#end","metadata":{"execution":{"iopub.status.busy":"2022-12-23T12:49:26.399351Z","iopub.execute_input":"2022-12-23T12:49:26.399996Z","iopub.status.idle":"2022-12-23T12:49:26.409184Z","shell.execute_reply.started":"2022-12-23T12:49:26.399960Z","shell.execute_reply":"2022-12-23T12:49:26.408108Z"},"trusted":true},"execution_count":null,"outputs":[]}]}